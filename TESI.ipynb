{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11be4ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import regex as re\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.datasets import make_blobs\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from TESI.APP import APosterioriaffinityPropagation as APP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5335fab9",
   "metadata": {},
   "source": [
    "## DATA CLEASING (can skip this part when you already have the clean file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95d5e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('DATI.csv', sep = \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a9444a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['Author', 'Year']] = data['Source'].str.extract(r'(.*?)\\s*\\((\\d{4})', expand=True)\n",
    "data = data.drop('Source', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0187c75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing unwanted strings\n",
    "data['Author'] = data['Author'].str.replace(\" \\(\", \"\")\n",
    "data['Year'] = data['Year'].str.replace(\" \\(\", \"\").str.replace(\" p. ...\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab11bdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6f865b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sort_values(by='Year', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9252c35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Year'] = data['Year'].replace('2005', '2006')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e16f791",
   "metadata": {},
   "outputs": [],
   "source": [
    "definition = data.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dca92a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = []\n",
    "for row in definition:\n",
    "    clean_row = row.lower()\n",
    "    clean_row = re.sub(r'\\([^)]*\\)', '', clean_row)\n",
    "    clean_row = re.sub(r'-', ' ', clean_row)\n",
    "    clean_row = clean_row.replace(\"‘\", \"\").replace(\"’\", \"\")\n",
    "    clean_rows = clean_row.replace(\"“\", \"\").replace(\"”\", \"\")\n",
    "    clean_text.append(clean_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ac9e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = list(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62b494e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_upgraded = []\n",
    "for text in texts:\n",
    "    text = text.replace(' ce ', ' circular economy ')\n",
    "    text_upgraded.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031043f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for text in text_upgraded:\n",
    "    if ' ce ' in text:\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af78be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Clean Definitions'] = text_upgraded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc68014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated dataset in a csv file\n",
    "data.to_csv('data_tesi.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74abcaf",
   "metadata": {},
   "source": [
    "## Upload the clean dataset (once you have the embeddings file you can skip this part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed12d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data_tesi.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c99102",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['Year'] == 2017]['Clean Definitions'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350fe421",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create lists of definitions for each year\n",
    "clust = {}\n",
    "\n",
    "for year, defin in zip(data['Year'], data['Clean Definitions']):\n",
    "    if year in clust:\n",
    "        clust[year].append(defin)\n",
    "    else:\n",
    "        clust[year] = [defin]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df84ff4",
   "metadata": {},
   "source": [
    "# Extract BERT embeddings for 'circular economy' (once you have the embeddings file you can skip this part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef466329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a BertEmbedder class. The class will create embeddings for each indexed target expression 'circular economy', and compute the average\n",
    "class BERTEmbedder:\n",
    "    def __init__(self, model_name='bert-base-uncased'):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "    def get_embeddings(self, texts, target_phrase):\n",
    "        phrase_words = target_phrase.split()\n",
    "        embeddings = []\n",
    "\n",
    "        for text in texts:\n",
    "            encoded_inputs = self.tokenizer.encode_plus(\n",
    "                text,\n",
    "                return_tensors='pt',\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                max_length=512\n",
    "            )\n",
    "            outputs = self.model(**encoded_inputs)\n",
    "            last_hidden_state = outputs.last_hidden_state\n",
    "            tokens = self.tokenizer.convert_ids_to_tokens(encoded_inputs[\"input_ids\"][0])\n",
    "            \n",
    "            # Check for the presence of the target phrase and extract the matching embeddings\n",
    "            phrase_embeddings = self._extract_phrase_embeddings(last_hidden_state, tokens, phrase_words)\n",
    "            embeddings.extend(phrase_embeddings)\n",
    "\n",
    "        return embeddings\n",
    "    #Extract the average embedding for 'circular economy'\n",
    "    def _extract_phrase_embeddings(self, hidden_states, tokens, phrase_words):\n",
    "        phrase_embeddings = []\n",
    "        for i in range(len(tokens) - len(phrase_words) + 1):\n",
    "            if tokens[i:i+len(phrase_words)] == phrase_words:\n",
    "                phrase_embedding = torch.mean(hidden_states[0, i:i+len(phrase_words)], dim=0)\n",
    "                phrase_embeddings.append(phrase_embedding)\n",
    "        return phrase_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528c6498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the embeddings for the target phrase\n",
    "def prepare_embeddings_by_year(clust_dict, target_phrase):\n",
    "    embedder = BERTEmbedder()\n",
    "\n",
    "    embeddings_by_year = {}\n",
    "    for year, texts in clust_dict.items():\n",
    "        embeddings_by_year[year] = embedder.get_embeddings(texts, target_phrase)\n",
    "\n",
    "    return embeddings_by_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885bf12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_phrase = 'circular economy'\n",
    "embeddings_by_year = prepare_embeddings_by_year(clust, target_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfac022",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_by_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2584d6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SAVE FILE\n",
    "with open('TESI/embeddings_by_year.pkl', 'wb') as file:\n",
    "     pickle.dump(embeddings_by_year, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0639b83c",
   "metadata": {},
   "source": [
    "## Upload the embeddings for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e0272ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## OPEN FILE\n",
    "with open('TESI/embeddings_by_year.pkl', 'rb') as file:\n",
    "      embeddings_by_year = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfcdf51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = [embedding.detach().numpy().flatten() for year in embeddings_by_year.keys() for embedding in embeddings_by_year[year]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01a6250c",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = {}\n",
    "for year in embeddings_by_year.keys():\n",
    "    word_embeddings[year] = [embedding.detach().numpy().flatten() for embedding in embeddings_by_year[year]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "89d9fd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "for embedding in word_embeddings[2006]:\n",
    "    print(type(embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7905db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n"
     ]
    }
   ],
   "source": [
    "for year in word_embeddings.keys():\n",
    "    for embedding in word_embeddings[year]:\n",
    "        print(embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088dd85b",
   "metadata": {},
   "source": [
    "## Incremental Clustering WiDiD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "663df906",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SENZA TRIMMING FACTOR\n",
    "def cluster_word_embeddings_aff_prop(word_embeddings, random_state=5):\n",
    "    clustering = AffinityPropagation(random_state=random_state).fit(word_embeddings)\n",
    "    labels = clustering.labels_\n",
    "    counts = Counter(labels)\n",
    "    exemplars = clustering.cluster_centers_\n",
    "    return labels, exemplars\n",
    "\n",
    "def incremental_affinity_propagation(embeddings_per_year, random_state=5):\n",
    "    year_to_centroids = {}\n",
    "    year_to_labels = {}\n",
    "    \n",
    "    for year, embeddings in sorted(embeddings_per_year.items()):\n",
    "        if year == min(embeddings_per_year.keys()):\n",
    "            # 2006 standard AP\n",
    "            labels, centroids = cluster_word_embeddings_aff_prop(embeddings, random_state=random_state)\n",
    "        else:\n",
    "            # Following years, APP\n",
    "            # Precedent embeddings are replaced by their centroids\n",
    "            previous_centroids = year_to_centroids[year - 1]\n",
    "            cumulative_embeddings = np.vstack((previous_centroids, embeddings))\n",
    "            \n",
    "            # Run AP again on the combined set of previous centroids and current embeddings\n",
    "            labels, new_centroids = cluster_word_embeddings_aff_prop(cumulative_embeddings, random_state=random_state)\n",
    "            \n",
    "            # Since we have a new set of centroids, we need to separate them into those that correspond to previous centroids (which will be discarded)\n",
    "            # and those that correspond to the current year\n",
    "            #THIS STEP MIGHT BE WRONG\n",
    "            centroids = new_centroids[len(previous_centroids):]\n",
    "            \n",
    "        # Store labels and centroids for the current year\n",
    "        year_to_centroids[year] = centroids\n",
    "        year_to_labels[year] = labels\n",
    "        \n",
    "    return year_to_labels, year_to_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d4b9d1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_to_labels, year_to_centroids = incremental_affinity_propagation(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8faa03b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2006: array([0, 1, 1, 1], dtype=int64),\n",
       " 2007: array([0, 1, 1, 1, 1, 2, 3, 3, 3], dtype=int64),\n",
       " 2008: array([0, 1, 2, 1, 1, 2, 3, 2, 2, 3, 3, 3, 3, 3, 4, 1, 4, 2, 4, 4, 4],\n",
       "       dtype=int64),\n",
       " 2009: array([0, 0, 0, 0, 0, 0, 0, 1, 0, 0], dtype=int64),\n",
       " 2010: array([0, 0, 3, 1, 1, 1, 1, 2, 3, 3, 0], dtype=int64),\n",
       " 2011: array([3, 3, 5, 1, 1, 5, 0, 1, 1, 1, 2, 3, 4, 4, 3, 5, 5], dtype=int64),\n",
       " 2012: array([0, 1, 3, 3, 3, 2, 3, 3, 3], dtype=int64),\n",
       " 2013: array([0, 3, 3, 0, 0, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3], dtype=int64),\n",
       " 2014: array([3, 2, 2, 3, 3, 2, 0, 3, 3, 4, 1, 1, 4, 4, 2, 3, 2, 3, 3, 4, 3, 3,\n",
       "        5, 5, 4], dtype=int64),\n",
       " 2015: array([4, 1, 3, 0, 2, 4, 1, 1, 4, 1, 2, 3, 4, 4, 3, 5, 5, 5, 2, 3],\n",
       "       dtype=int64),\n",
       " 2016: array([4, 5, 0, 0, 0, 5, 4, 1, 2, 5, 0, 3, 3, 3, 4, 4, 3, 5, 5, 5],\n",
       "       dtype=int64),\n",
       " 2017: array([7, 0, 5, 2, 0, 7, 0, 1, 5, 0, 2, 2, 2, 2, 5, 2, 7, 3, 3, 3, 0, 4,\n",
       "        4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 5, 3, 7, 5, 6, 7, 7, 5, 5, 2, 7, 5,\n",
       "        2], dtype=int64)}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year_to_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7545e8f",
   "metadata": {},
   "source": [
    "## Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f6c1fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44a4187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517f633b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
