{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca6c91df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models.phrases import Phrases, Phraser, ENGLISH_CONNECTOR_WORDS\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import enchant\n",
    "from time import time \n",
    "import multiprocessing\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "205c17e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = np.load('tokens_per_year.npy',allow_pickle='FALSE').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7e27c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#documents Ã¨ la lista di liste\n",
    "#doc sono le liste interne\n",
    "def build_bigram_dictionary(documents, min_count, threshold):\n",
    "    phrases = Phrases(documents, min_count=min_count, threshold=threshold, connector_words=ENGLISH_CONNECTOR_WORDS)\n",
    "    bigram = Phraser(phrases)\n",
    "    tokens_text_with_bigrams = [bigram[doc] for doc in documents]\n",
    "    return tokens_text_with_bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8114da",
   "metadata": {},
   "source": [
    "**TRAIN WORD2VEC MODEL ON THE WHOLE SET**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f8d316a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A VOCABULARY OF BIGRAMS THE DATASET PER YEAR\n",
    "bigram_vocab = {}\n",
    "for year, tokens in tokens.items():\n",
    "    tokens_per_year = build_bigram_dictionary(tokens, 11, 20)\n",
    "    bigram_vocab[year] = tokens_per_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19000e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "wholeset =  [doc for papers in bigram_vocab.values() for doc in papers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76bbae58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20901"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = defaultdict(int)\n",
    "for big in wholeset:\n",
    "    for i in big:\n",
    "        word_freq[i] += 1\n",
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1661c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(word_freq, key=word_freq.get, reverse=True)[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ee75be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f57bcf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this first step, we set up the parameters of the model one-by-one. we don't supply the parameter sentences, and therefore leave the model uninitialized, purposefully.\n",
    "w2v_model = Word2Vec(sg = 1,\n",
    "                     vector_size = 150,\n",
    "                     min_count=10, #Ignores all words with total absolute frequency lower than this - (2, 100)                    \n",
    "                     window=10,  #The maximum distance between the current and predicted word within a sentence. E.g. window words on the left and window words on the right of our target\n",
    "                     sample=6e-5,   #Dimensionality of the feature vectors\n",
    "                     alpha=0.03,  #The initial learning rate\n",
    "                     min_alpha=0.0007,  #Learning rate will linearly drop to min_alpha as training progresses. To set it: alpha - (min_alpha * epochs) ~ 0.00\n",
    "                     negative=5,  #If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\" should be drown. If set to 0, no negative sampling is used. - (5, 20)\n",
    "                     workers=cores-1  #Use these many worker threads to train the model (=faster training with multicore machines)\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1952080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.01 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "#Here it builds the vocabulary from a sequence of sentences and thus initialized the model. With the loggings, We can follow the progress and even more important, the effect of min_count and sample on the word corpus. We noticed that these two parameters, and in particular sample, have a great influence over the performance of a model. Displaying both allows for a more accurate and an easier management of their influence.\n",
    "#Word2Vec requires us to build the vocabulary table (simply digesting all the words and filtering out the unique words, and doing some basic counts on them)\n",
    "\n",
    "w2v_model.build_vocab(wholeset)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ccceb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 1.25 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "w2v_model.train(wholeset, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1febf586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# salva il modello \n",
    "# word2vec_sg modello con 10 negativity\n",
    "# word2vec_sg2 modello con 5 negativity\n",
    "model = w2v_model.save(\"word2vec_sg2.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4016d9",
   "metadata": {},
   "source": [
    "**FINETUNING OF THE MODEL ON THE DATASET OF EACH YEAR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "131f53d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"word2vec_sg2.model\")\n",
    "\n",
    "ft_models = {}\n",
    "for year, documents in bigram_vocab.items():\n",
    "    \n",
    "    # Upload the deepcopy of the existing model each time the loop is restarted (for each year)\n",
    "    ft_model = deepcopy(model)\n",
    "    \n",
    "    # Update the dictionary with the bigrams of the year and train the new model\n",
    "    ft_model.build_vocab(documents, update=True)\n",
    "    ft_model.train(documents, total_examples=ft_model.corpus_count, epochs=30)\n",
    "    \n",
    "    # Save the model\n",
    "    ft_model.save(f\"word2vec_{year}_sg2.model\")\n",
    "    ft_models[year] = ft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7702d49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE THE VECTORS:\n",
    "ft_vectors = {}\n",
    "for year, model in ft_models.items():\n",
    "    word_vectors = model.wv  \n",
    "    ft_vectors[year] = word_vectors"
   ]
  }
