{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "067ce6eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "import fitz\n",
    "import os\n",
    "from pdfminer.high_level import extract_text\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import string\n",
    "import copy\n",
    "import regex as re\n",
    "import unstructured\n",
    "from unstructured.cleaners.core import replace_unicode_quotes, clean_non_ascii_chars\n",
    "import enchant\n",
    "from TxtProcessing2 import TextPreprocessing as tp\n",
    "from TxtProcessing2 import TokenProcessor as tkp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c3bf80",
   "metadata": {},
   "source": [
    "# Upload the papers & Cleaning steps\n",
    "\n",
    "The scope of this notebook is to perform several pre-processing steps in order to remove noise from the text that could lead to undesireable results in training Word2Vec. TextPreprocessing is a class containing all pre-processing and cleaning functions.\n",
    "\n",
    "**Cleaning steps:**\n",
    "1. Mantain only the core text for each papers (i.e. remove all text before 'Abstract' and all text after 'References' or 'Results').\n",
    "2. Remove any 'formatting' character.\n",
    "3. Remove punctuation.\n",
    "4. Remove numbers (for the purpose of the project we don't need numbers).\n",
    "5. Remove stopwords.\n",
    "6. Tokenize and lemmatize text.\n",
    "\n",
    "One final step is to check whether the tokens correspond to real words in the english language. Three cases are evaluated:\n",
    "1. When the token is a real word.\n",
    "2. When the union of two consecutive tokens represents a word.\n",
    "3. When the split of two or more consecutive tokens represents two or more words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c15e692",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Desktop/Università/TESI/training_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3b833b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = tp(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424dc9f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "papers = tp.extract_text_from_pdfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495c50f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b36acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "np.save('papers_per_year.npy', papers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a37ba7",
   "metadata": {},
   "source": [
    " **Upload the extracted file**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95b9fa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "papers_per_year = np.load('papers_per_year.npy',allow_pickle='FALSE').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f84487",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_per_year.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1300daff",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = ['REFERENCES', 'R E F E R E N C E S', 'r e f e r e n c e s']\n",
    "wordtoreplace = 'References'\n",
    "papers_per_year = tp.restructure_words(papers_per_year,wordlist, wordtoreplace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c502b220",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = ['ABSTRACT', 'A B S T R A C T', 'a b s t r a c t']\n",
    "wordtoreplace = 'Abstract'\n",
    "new_papers = tp.restructure_words(papers_per_year, wordlist, wordtoreplace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65927ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = ['RESOURCES', 'R E S O U R C E S', 'r e s o u r c e s']\n",
    "wordtoreplace = 'Resources'\n",
    "new_papers = tp.restructure_words(new_papers, wordlist, wordtoreplace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7b71d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = ['INTRODUCTION', 'I N T R O D U C T I O N', 'i n t r o d u c t i o n']\n",
    "wordtoreplace = 'Introduction'\n",
    "new_papers = tp.restructure_words(new_papers, wordlist, wordtoreplace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e935644",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for year, papers in papers_per_year.items(): \n",
    "    for text in papers: \n",
    "        if 'REFERENCES' in text: \n",
    "            count += 1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fef66a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_papers = tp.extract_core(new_papers, 'Abstract', 'References')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e22ce8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_papers = tp.remove_end_from_txt(new_papers, 'Resources')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275c02dc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_papers = tp.remove_end_from_txt(new_papers, 'Copyright')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa40ace7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_paperss= tp.remove_empty_text(new_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979a4e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_paperss = tp.unite_segmented_words(new_paperss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aae28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [\n",
    "    r'\\x0c?\\s*(Page\\s+\\d+.*?\\n+|Copyright © [^\\x0c]+?\\x0c\\d+\\n\\n.+?\\n\\n)', \n",
    "    r'\\n(?:See the Terms and Conditions[^\\n]+|by University Of[^\\n]+|OA articles are governed[^\\n]+)\\n',  \n",
    "    r'(?:\\* Corresponding author\\..+?\\(C\\. Gao\\)\\.)',\n",
    "    r'((?:[0-9]{4}-[0-9]+\\/\\$.*?\\sltd\\.)|(E-mail address:.+?\\(C\\. Gao\\)\\.))',  \n",
    "    r'\\b(Wiley|ERP Environment|Sust Dev)\\b[\\s\\d–-]+',  \n",
    "    r'Downloaded from .+?(?=See the Terms and Conditions)',  \n",
    "    r'See the Terms and Conditions on .+? for rules of use',  \n",
    "    r'\\b[e|i] chapter\\b',\n",
    "    r'(?i)\\bet\\s+al\\.\\b',  \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38bf73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_list = ['\\n', '\\t', '\\x0c','‘','’', '“', '”', '©', 'et. al', 'cid', 'doi', 'DOI', '—']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5cdf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_paper = tp.comprehensive_cleaning(new_paperss, replace_list, patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13aa79e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for liste in new_paper.values():\n",
    "    for i, lista in enumerate(liste):\n",
    "        if 'cid' in lista:\n",
    "            x = lista.replace('cid', '')\n",
    "            new_paper[year][i] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d35e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for liste in new_paper.values():\n",
    "    for i, lista in enumerate(liste):\n",
    "        if 'cid' in lista:\n",
    "            count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92980b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_tokens = {}\n",
    "\n",
    "for year, paper in new_paper.items():\n",
    "    for i, txt in enumerate(paper):\n",
    "        if 'circulareconomy' in txt:\n",
    "            new_txt = txt.replace('circulareconomy', 'circular economy')\n",
    "        else:\n",
    "            new_txt = txt\n",
    "        pre_tokens.setdefault(year, []).append(new_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d83fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year, paper in pre_tokens.items():\n",
    "    for i, txt in enumerate(paper):\n",
    "        word = txt.split()\n",
    "        for j in range(len(word) - 1): \n",
    "            if word[j] == 'cir' and word[j + 1] == 'cular':\n",
    "                word[j:j+1] = 'circular'\n",
    "        \n",
    "        pre_tokens[year][i] = ' '.join(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50eda95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year, paper in pre_tokens.items():\n",
    "    for i, txt in enumerate(paper):\n",
    "        word = txt.split()\n",
    "        for j in range(len(word) - 1): \n",
    "            if word[j] == 'sus' and word[j + 1] == 'tainable':\n",
    "                word[j:j+1] = 'sustainable'\n",
    "        pre_tokens[year][i] = ' '.join(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2de3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_text=tp.tokenize_and_lemmatize(pre_tokens, 'lemma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7629ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesi = ['january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b031fd09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "processor = tkp(token_text, mesi)\n",
    "nuovo_token_text = processor.process_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac487f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in nuovo_token_text:\n",
    "    nuovo_token_text[year] = [[word for word in sub_list if len(word) >= 3] for sub_list in nuovo_token_text[year]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f480f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in nuovo_token_text:\n",
    "    nuovo_token_text[year] = [[word.replace('-', '') for word in sub_list] for sub_list in nuovo_token_text[year]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb70e744",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for year, liste in nuovo_token_text.items():\n",
    "    for lista in liste:\n",
    "        if '-' in lista:\n",
    "            count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ce4853",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year, papers in nuovo_token_text.items():\n",
    "    for i,lista in enumerate(papers):\n",
    "        for word in lista:\n",
    "            if len(word) < 3:\n",
    "                print(i, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc62f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for year, papers in nuovo_token_text.items():\n",
    "    for i,lista in enumerate(papers):\n",
    "        for j, word in enumerate(lista):\n",
    "            if 'circulareconomy' in word:\n",
    "                count +=1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad89310",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year, papers in token_text.items():\n",
    "    for i,lista in enumerate(papers):\n",
    "        for word in lista:\n",
    "            if len(word) < 3:\n",
    "                print(i, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33fad2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57995423",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_list = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824b5db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tokens:\n",
    "for year, liste in nuovo_token_text.items():\n",
    "    for i, lista in liste:\n",
    "        new_words = [word for word in lista if word not in stop_words_list]\n",
    "        final_tokens[year][i] = new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7269f7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tokens = {}\n",
    "\n",
    "for year, liste in nuovo_token_text.items():\n",
    "    final_tokens[year] = []\n",
    "    for i, lista in enumerate(liste):  \n",
    "        new_words = [word for word in lista if word not in stop_words_list]\n",
    "        final_tokens[year].append(new_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93d7c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('tokens_per_year.npy', final_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
